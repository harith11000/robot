from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import pandas as pd
import numpy as np
from datetime import datetime,timedelta 
import json
import os
import csv
import firebase_admin
from firebase_admin import db, credentials
from deep_translator import GoogleTranslator

from main.ASetting import mark_city, access_token

import random, time
from selenium.webdriver.common.action_chains import ActionChains

import json
import subprocess
from pathlib import Path
import sys

import platform
from selenium_stealth import stealth

import requests

#region ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏ö

#‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏¥‡πâ‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
sys.stdout.reconfigure(line_buffering=True)

#‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
this_system = platform.system().lower()

#‡πÄ‡∏≠‡∏≤‡πÉ‡∏ß‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏£‡∏±‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
flag_news = "/home/mir/robot/news.flag"

#endregion

#region FIREBASE

## authenticate to firebase
cred = credentials.Certificate("fire_base_admin.json")

firebase_admin.initialize_app(cred, {"databaseURL": "https://robot-mir-79bc9-default-rtdb.asia-southeast1.firebasedatabase.app/"})

# creating reference to root node
ref = db.reference("/")


#endregion

#‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
articles = []

def send_line_message(token, text):
    url = "https://api.line.me/v2/bot/message/broadcast"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}"
    }

    payload = {
        "messages": [
            {"type": "text", "text": text}
        ]
    }

    #‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    response = requests.post(url, headers=headers, data=json.dumps(payload))

def make_old_kw(): #‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ ‡∏£‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ

    from main.ASetting import base_keywords

    try :
        #‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô ‡∏•‡∏á text
        log_kw = open('log_keywords'+'.csv','a')

    except :
        log_kw = open('log_keywords'+'.csv','w')
        
    catagory = list(base_keywords.keys())
    for k in range(len(catagory)) :
    
        key = catagory[k]

        if k == 0 : 
            log_kw.writelines(key+'\n')
        else : 
            log_kw.writelines('\n'+key+'\n')


        kw_score = list(base_keywords[key].items())

        for kws in kw_score :

            #log_kw.writelines(str(kws[0])+','+str(kws[1])+'\n')
            log_kw.writelines(str(kws[0])+','+str(kws[1])+'\n')

    log_kw.close()

def update_old_kw(update_keywords): 

    log_kw = open('log_keywords'+'.csv','w')
        
    catagory = list(update_keywords.keys())

    for k in range(len(catagory)) :
    
        key = catagory[k]

        if k == 0 : 
            log_kw.writelines(key+'\n')
        else : 
            log_kw.writelines('\n'+key+'\n')


        kw_score = list(update_keywords[key].items())

        for kws in kw_score :

            #log_kw.writelines(str(kws[0])+','+str(kws[1])+'\n')
            log_kw.writelines(str(kws[0])+','+str(kws[1])+'\n')

    log_kw.close()

def read_kw():

    rec_kw = {}
    mark_last_kw = [] #‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏π‡∏ß‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏´‡∏ô

    read_log = open("log_keywords.csv",'r').readlines()

    for rl in read_log :

        data = rl.split('\n')[0]
        
        if data != '' :
            if (',' not in data) : 
                rec_kw[data] = {}
                mark_last_kw.append(data)

            elif (',' in data) :
                sep_data = str(data).split(',')
                
                key = sep_data[0]
                val = sep_data[1]

                last_kw = mark_last_kw[-1]

                rec_kw[last_kw][key] = float(val)

    return rec_kw

def scroll_page_smooth(driver, step=500, delay=1, max_scrolls=20, timeout=5):
   
    """
    ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏•‡∏á‡∏ó‡∏µ‡∏•‡∏∞ step px ‡∏à‡∏ô‡∏™‡∏∏‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏£‡∏ö max_scrolls
    timeout = ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Ç‡∏¢‡∏±‡∏ö (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
    """
    #"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠...
    last_height = driver.execute_script("return document.body.scrollHeight")
    still_count = 0  # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á
   

    for i in range(max_scrolls):
        driver.execute_script(f"window.scrollBy(0, {step});")
        time.sleep(delay)
        
        new_height = driver.execute_script("return document.body.scrollHeight")
        
        if new_height == last_height:
            still_count += 1
            if still_count * delay >= timeout:
                #(f"‚úÖ ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô: ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô {timeout} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
                break
        else:
            still_count = 0  # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á

        #(f"‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {i+1}: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á {new_height}")
        last_height = new_height
    
    #("üèÅ ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß")

def analyze_title(title, keywords):
    keep_point = 0.0001
    score = 0
    matched = []        
          
    #‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏ó‡∏µ‡πà 2/3
    list_key = list(keywords.keys())

    #catagory news ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏• keywords
    for list_catagory in list_key:
        
        dict_wordr_org = keywords[list_catagory]

        #‡∏õ‡∏£‡∏±‡∏ö keywords ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        dict_words_small = {k.lower(): in_catagory_org for k, in_catagory_org in dict_wordr_org.items()}

        kw_use_catagory = list(dict_words_small.keys())



        #--------------‡∏£‡∏ß‡∏° keyword ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏ô‡∏Ç‡πà‡∏≤‡∏ß ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤ set‡πÉ‡∏ß‡πâ
        title_single = (title.lower()).split(' ') #kwyword ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

        #‡∏ö‡∏±‡∏ô‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏ß‡∏° ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÉ‡∏ô list
        same_one_kw = list(set(title_single) & set(kw_use_catagory))
       
        for match_one_kw in same_one_kw :
            
            #‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤ ‡πÅ‡∏•‡πâ‡∏ß + 0.0001 ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏≠‡∏á
            value_up = float('%.4f'%((dict_words_small[match_one_kw]) + keep_point))

            #‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏ó‡∏µ‡πà dick
            keywords[list_catagory][match_one_kw] = value_up
            
            #‡∏£‡∏ß‡∏° score
            score += value_up

            #‡∏£‡∏ß‡∏° word ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
            matched.append(match_one_kw+str(value_up))
        


        #--------------‡∏£‡∏ß‡∏° keyword ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏π‡πà ‡πÉ‡∏ô‡∏Ç‡πà‡∏≤‡∏ß ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤ set‡πÉ‡∏ß‡πâ
        title_two = []
        for m in range(len(title_single)-1): #-1‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ 12 23 34 45
            title_two.append(title_single[m]+' '+title_single[m+1])
            
        same_two_kw = list(set(title_two) & set(kw_use_catagory))

        for match_two_kw in same_two_kw :
            
            value_up = float('%.4f'%((dict_words_small[match_two_kw]) + keep_point))

            #‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏ó‡∏µ‡πà dick
            keywords[list_catagory][match_two_kw] = value_up
            
            #‡∏£‡∏ß‡∏° score
            score += value_up

            #‡∏£‡∏ß‡∏° word ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
            matched.append(match_two_kw+str(value_up))

    joint = ' '.join(matched)

    return score, joint, keywords

def find_head(soup_in,script,typex,main,sub) :
    
    ''' ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    for head in soup.find_all("script", type="application/ld+json"):

    try:
        print(4.1)
        data_in = json.loads(head.string)
        data_head = data_in['mainEntity']['itemListElement']
        print(4.2)
        for dx in data_head :
            url = str(dx['url'])
            clean_url = url.replace("https://www.reuters.com/", "")

            clean_slash = clean_url.split('/')


            if len(clean_slash) == 3 :
                clean_text = clean_slash[1]

            elif len(clean_slash) == 4 :
                clean_text = clean_slash[2]

        
            # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß ‡∏Å‡∏±‡∏ö ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
            text_and_date = clean_text.split('-')

            #‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡πà‡∏≤‡∏ß
            time_text = '-'.join(text_and_date[-3:])
            time_news = datetime.strptime(str(time_text+' 00:00:00'), '%Y-%m-%d  %H:%M:%S') 

            #‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß
            title_range = text_and_date[:-3]
            title_text = ' '.join(title_range)

            if title_text not in articles :
                articles.append(time_news)
                articles.append(title_text)
                articles.append(url)
        print(4.3)


        count_news_in = data_in['mainEntity']['numberOfItems']
        no_news += count_news_in
        print(f'number news == {count_news_in}')

    except :
        continue
    '''

    for head in soup_in.find_all(script, type=typex):
    
        try:
            
            data_in = json.loads(head.string)
            data_head = data_in[main][sub]

        except :
            continue
    
    return data_head

def clean_text(a,b):

    a_no_stop = a.replace('.', ' ')
    a_no_com = a_no_stop.replace(',', '')
    a_no_cod = a_no_com.replace('"', '')

    b_no_stop = b.replace('.', ' ')
    b_no_com = b_no_stop.replace(',', '')
    b_no_cod = b_no_com.replace('"', '')

    good_text = a_no_cod+' , '+b_no_cod

    return good_text

def open_book(book_name) : #‡∏ü‡∏±‡∏á‡∏ä‡∏±‡πà‡∏ô‡∏ô‡∏µ‡πâ‡∏ô‡∏µ‡πâ‡πÄ‡∏≠‡∏≤‡πÉ‡∏ß‡πâ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î ‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ
   
    #‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ + ‡πÄ‡∏õ‡∏¥‡∏î ‡∏´‡∏ô‡πâ‡∏≤ bookmark
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡∏´‡∏≤
    TARGET_TITLE = book_name  # ‡∏ä‡∏∑‡πà‡∏≠ bookmark ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ

    # ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤ Chromium ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏Å‡πà‡∏≠‡∏ô
    bookmark_paths = [
        Path.home() / ".config" / "chromium" / "Default" / "Bookmarks",
        Path.home() / ".config" / "chromium-browser" / "Default" / "Bookmarks",
        Path.home() / ".config" / "google-chrome" / "Default" / "Bookmarks",
    ]

    bookmarks_file = None
    for p in bookmark_paths:
        if p.exists():
            bookmarks_file = p
            break

    if not bookmarks_file:
        print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Bookmarks ‡∏Ç‡∏≠‡∏á Chromium ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡πÑ‡∏ß‡πâ.")
        print("‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö Bookmarks ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÉ‡∏ô bookmark_paths.")
        sys.exit(1)

    with open(bookmarks_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ bookmark ‡πÅ‡∏ö‡∏ö recursive (‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå)
    def find_bookmark(node, title):
        results = []
        t = node.get("type")
        if t == "url" and node.get("name") == title:
            return [node.get("url")]
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô folder ‡∏à‡∏∞‡∏°‡∏µ children
        for key in ("children",):
            if key in node:
                for child in node[key]:
                    results += find_bookmark(child, title)
        # ‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô roots
        for k in ("roots",):
            if k in node:
                for root_key in node[k]:
                    root = node[k][root_key]
                    results += find_bookmark(root, title)
        return results

    urls = find_bookmark(data, TARGET_TITLE)

    if not urls:
        print(f"‡πÑ‡∏°‡πà‡∏û‡∏ö bookmark ‡∏ä‡∏∑‡πà‡∏≠ '{TARGET_TITLE}' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {bookmarks_file}")
        sys.exit(1)

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å url ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏û‡∏ö
    url_to_open = urls[0]
    print("‡πÄ‡∏õ‡∏¥‡∏î:", url_to_open)

    # ‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ chromium profile ‡πÄ‡∏î‡∏¥‡∏° (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ cookies / session)
    CHROMIUM_BIN = "/usr/bin/chromium"  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
    USER_DATA_DIR = str(Path.home() / ".config" / "chromium")  # ‡πÉ‡∏ä‡πâ profile ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    PROFILE_DIR = "Default"  # ‡∏´‡∏£‡∏∑‡∏≠ Profile 1, Profile 2 ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå

    cmd = [
        CHROMIUM_BIN,
        f"--user-data-dir={USER_DATA_DIR}",
        f"--profile-directory={PROFILE_DIR}",
        url_to_open
    ]

    # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏õ‡∏¥‡∏î headless/automation ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏™‡πà --headless
    subprocess.Popen(cmd)

def get_data(driver, news_loop_round):

    loading = True

    count_high = []

    while loading == True :
        
        #region---------------------‡∏ó‡∏≥‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå

        try :
           
            # small mouse move (ActionChains)
            actions = ActionChains(driver)
            actions.move_by_offset(random.randint(10,300), random.randint(10,200)).perform()
          
            time.sleep(random.uniform(0.5,2))

            #‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏à‡∏≠‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            scroll_page_smooth(driver, step=300, delay=random.uniform(1,2), max_scrolls=5)
            time.sleep(random.uniform(3,6))
          
            #---------------------------------------------
           
            #random ‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏ö‡∏£‡∏Ñ
            time.sleep(random.uniform(2,5))

            # small mouse move (ActionChains)
            actions = ActionChains(driver)
            actions.move_by_offset(random.randint(10,200), random.randint(10,100)).perform()

            #random ‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏ö‡∏£‡∏Ñ
            time.sleep(random.uniform(2,5))

        except :
            pass


        #endregion
        
        #region----------------------‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

        print(f'......Get news', flush=True)
        
        try :

            # ‡∏£‡∏≠‡πÉ‡∏´‡πâ FeedListItem ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ç‡πà‡∏≤‡∏ß‡πÇ‡∏´‡∏•‡∏î
            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "li[data-testid='FeedListItem']")))
        
        except :
            print(f'.........Bad Get news...', flush=True)
            loading = False
            break

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î            
        html = driver.execute_script("return document.body.innerHTML;")
        soup = BeautifulSoup(html, 'html.parser')
        
        #‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πà‡∏≤‡∏ß
        feed_items = soup.select("li[data-testid='FeedListItem']")
        
        for item in feed_items:
            
            last_update = item.select_one("time") #24 mins ago
            lastx = last_update.get_text(strip=True)

            datetime_update = last_update["datetime"] # ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö '%Y-%m-%dT%H:%M:%S.%fZ'
            
            #‡∏°‡∏µ div ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ó‡∏ô data-testid='Title'
            title_tag = item.select_one("div[data-testid='Title']")
            title = title_tag.get_text(strip=True)
            
            desc_tag = item.select_one("p[data-testid='Description']")
            description = desc_tag.get_text(strip=True)

            url_tag = item.select_one("div[data-testid='Title']")
            url_tagx = url_tag.select_one("a")
            url_use = str(url_tagx['href'])

            #‡∏ï‡∏±‡∏î‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            text_sum = clean_text(title,description)


            if text_sum not in articles :
                
                try :time_news = datetime.strptime(str(datetime_update), '%Y-%m-%dT%H:%M:%SZ') 
                except : time_news = datetime.strptime(str(datetime_update),  '%Y-%m-%dT%H:%M:%S.%fZ') 
                
                timestamp =  ( (time_news) + timedelta(hours=7) ).replace(microsecond=0)

                articles.append(str(timestamp))
                articles.append(lastx) 
                articles.append(text_sum)
                articles.append('https://www.reuters.com'+url_use)

        print(f'.........Good...', flush=True)
   
        

        #endregion     
        
        #region-----------------------‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠ loop ‡∏Ñ‡∏£‡∏ö ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        #‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Åloop ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ ‡πÄ‡∏≠‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ ‡∏Ç‡πà‡∏≤‡∏ß ‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤
        if news_loop_round  == 0 :
            print(f'......End setting page', flush=True)
            break
         #endregion
   
        #region-----------------------‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ ‡∏•‡∏á‡∏•‡πà‡∏≤‡∏á
        print(f'......Scroll news', flush=True)
        scroll_page_smooth(driver)
        
        #random ‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏ö‡∏£‡∏Ñ
        time.sleep(random.uniform(3,8))

        #endregion
      
        #region-----------------------‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏õ‡∏∏‡πà‡∏° ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°
        print(f'......Put load_more or stop', flush=True)

        try:
    
            load_more = driver.find_element(By.CSS_SELECTOR, 'button[data-testid="FeedContentLoadMore"]')
            
            driver.execute_script("arguments[0].scrollIntoView(true);", load_more)

            #random ‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏ö‡∏£‡∏Ñ
            time.sleep(random.uniform(2,5))

            driver.execute_script("arguments[0].click();", load_more)
            
        except :

            last_height = driver.execute_script("return document.body.scrollHeight")

            if len(count_high) == 0 : #‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏π‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
                count_high.append(int(last_height))
                continue

            else :
                if last_height != count_high[-1] : # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á ‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡πà‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏µ‡∏Å
                    continue
                else : # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ / ‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß
                    break
            
        

        #endregion

        news_loop_round -= 1

    return loading 

def main():
    # --------------------------------------------------------
    # ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ key words ‡∏à‡∏≤‡∏Å ‡πÑ‡∏ü‡∏•‡πå text
    last_keywords = read_kw()

    #‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏ô‡∏±‡∏ö ‡∏ã‡πâ‡∏≥
    news_log = 'log_news.csv'
    try :
        get_news_log = []
        with open(news_log, 'r', encoding='utf-8') as f:
            for line in f:
                get_news_log.append(line.strip())
   
    except :
        get_news_log = []
        with open(news_log, 'w', encoding='utf-8') as f:
            pass

    #‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ
    #today_utc = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)

    frist = True

    for city, count in mark_city.items() : 
        
        print('', flush=True)
        print(f'...Get news from {city}', flush=True)

        url_get = 'https://www.reuters.com/'+city
        
        options = Options()

        if "linux" in this_system :    
            CHROMEDRIVER_PATH = "/usr/bin/chromedriver"  # path ‡∏Ç‡∏≠‡∏á chromedriver ‡∏ö‡∏ô Pi
            CHROMIUM_BIN = "/usr/bin/chromium"           # path ‡∏Ç‡∏≠‡∏á chromium ‡∏ö‡∏ô Pi
           
            PROFILE_PATH = "/home/mir/chrome_profiles/reuters"
           
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            os.makedirs(PROFILE_PATH, exist_ok=True)

            options = webdriver.ChromeOptions()
            options.binary_location = CHROMIUM_BIN

            # üß© ‡πÉ‡∏ä‡πâ user-data-dir ‡πÄ‡∏î‡∏¥‡∏°
            options.add_argument(f"--user-data-dir={PROFILE_PATH}")
            options.add_argument("--profile-directory=Default")
            
            #options.headless = True
            #options.add_argument("--headless=new") 

            options.binary_location = CHROMIUM_BIN
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_argument("--disable-features=IsolateOrigins,site-per-process")
            options.add_argument("--ignore-certificate-errors")
            options.add_argument("--disable-extensions")
            options.add_argument("--start-maximized")

            #‡∏õ‡∏¥‡∏î automation flags
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option("useAutomationExtension", False)


            # new user-agent (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏à‡πâ‡∏≤‡∏ô‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏°‡∏≤)
            # new_ua ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏™‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏°‡∏≤
            new_ua = ("Mozilla/5.0 (X11; CrOS x86_64 14541.0.0) AppleWebKit/537.36 "
                    "(KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36")
            options.add_argument(f"user-agent={new_ua}")
        
            service = Service(CHROMEDRIVER_PATH)
            driver = webdriver.Chrome(service=service, options=options)


            stealth(driver,
                languages=["en-GB","en-US", "en"],
                vendor="Google Inc.",
                platform="Linux x86_64", 
                webgl_vendor= "Broadcom",
                renderer="ANGLE (Broadcom, V3D 4.2.14.0, OpenGL ES 3.1 Mesa 25.0.7-2+rpt3)",
                fix_hairline=True)

        elif "windows" in this_system :
            driver = webdriver.Chrome(options=options)

            stealth(driver,
                languages=["en-US", "en"],
                vendor="Google Inc.",
                platform="Win64",
                webgl_vendor="NVIDIA Corporation",
                renderer="NVIDIA GeForce GTX 1650/PCIe/SSE2",
                fix_hairline=True)
        
        # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ñ‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏´‡∏•‡∏î‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞ error
        driver.set_page_load_timeout(180)

        print(f'......Open news', flush=True)
    
        try :

            #‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å
            if frist == True :
                #‡πÄ‡∏õ‡∏¥‡∏î ‡πÄ‡∏ß‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
                driver.get('https://www.reuters.com')
                
                time.sleep(random.uniform(7,15))
                
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á scroll
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight/3);")
                time.sleep(random.uniform(7, 15))

                frist = False
        
        except : 
            pass

        #‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ß‡πá‡∏ö ‡∏ï‡∏≤‡∏°‡∏•‡∏¥‡πâ‡∏á
        driver.get(url_get)
        run_app = get_data(driver, count)

        #app run ‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏Å‡∏ï‡∏¥
        if run_app == True :
            driver.quit()  

        # ‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
        else :
            print(f'.........Stop by Verification Require LINE NOTICE', flush=True)
            send_line_message(access_token,'.........Stop Auto_News by Verification Require')
            break

    print(f'...Finish get news', flush=True)
    print(flush=True)

    #‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠
    if len(articles) != 0 :

        data_tran = np.array(articles).reshape(-1,4)
        print(f'...Analise news', flush=True)
    
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        comp_data = []
        log_news_update = []

        for ix in range(len(data_tran)):
            
            news = str(data_tran[ix][2])

            s, matches, update_words = analyze_title(news, last_keywords)

            #‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ score ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0 ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö
            if (news not in comp_data) and (s != 0) :
            
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏öfire base
                comp_data.append(data_tran[ix][0])
                comp_data.append(data_tran[ix][1])
                comp_data.append(news)
                comp_data.append(s)
                comp_data.append(matches)
                comp_data.append(data_tran[ix][3])


                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß ‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÉ‡∏ô log ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡πà‡∏≤‡∏ß ‡∏´‡∏£‡∏∑‡∏≠ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó keyword
                add_text = news.split(' ')

                keep_news = str(data_tran[ix][0])+' '+add_text[0]+' '+add_text[1]

                if (len(get_news_log) == 0) or (keep_news not in get_news_log) :
                    
                    #‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà
                    with open(news_log, 'a', encoding='utf-8') as f:
                        f.write(keep_news+"\n")

                    # update kwyword
                    for k in last_keywords:
                        last_keywords[k] = update_words[k]   

                #‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà ‡∏°‡∏µ score
                #‡πÄ‡∏Å‡πá‡∏ö log ‡πÑ‡∏ß‡πâ‡∏£‡∏≠‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó
                log_news_update.append(keep_news)





        #‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó ‡πÑ‡∏ü‡∏•‡πå log_news.csv  ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡πà‡∏≤ ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å ‡∏ö‡∏±‡∏ô‡∏ó‡∏Å‡πÑ‡∏ß‡πâ‡∏à‡∏∞‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
        with open(news_log, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            for row in log_news_update:
                writer.writerow([row])




        #‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ keywords ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó ‡πÑ‡∏ü‡∏•‡πå
        update_old_kw(last_keywords)


        # --------------------------------------------------------
        print(f'...Keep to Firebase', flush=True)

        #‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á
        out_data = (np.array(comp_data)).reshape(-1,6)
        df = pd.DataFrame(out_data, columns=['time','lastup','news','score','matches','url'])
       
        df['score'] = df['score'].astype(float)
        much_score = df['score'].nlargest(4).iloc[:4].tolist()


        #‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° score
        df = df.sort_values(by='time', ascending=False)

        db_news = {'000':['TimeTH','LastUP','News','Score','Matching','URL']}

        np_data =  df.to_numpy()

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ db
        for ni in range(len(np_data)) :

            if len(db_news) < 10 : dbn = '00'+str(len(db_news))
            
            else : dbn = '0'+str(len(db_news))
            
            score = float(np_data[ni][3])

            #‡∏™‡πà‡∏á‡πÑ‡∏•‡∏ô‡πå
            if score in much_score :
                
                #‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤
                input_text = str(np_data[ni][2]) 
                translated_text = GoogleTranslator(source='auto', target='th').translate(input_text)

                text = 'score '+str('%.4f'%score)+' '+translated_text
                send_line_message(access_token,text)

            #fierbases
            db_news[dbn] = [str(np_data[ni][0]), str(np_data[ni][1]), str(np_data[ni][2]), score, str(np_data[ni][4]), str(np_data[ni][5])]

        db.reference('/News_analise').set(db_news)

        
        #‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ ‡∏£‡∏±‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
        if "linux" in this_system :
            print(f'...Make flag_news', flush=True) 
            open(flag_news, "w").close() 

#‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏¥‡πâ‡∏ô‡∏≠‡∏≠‡∏Å log
main()


